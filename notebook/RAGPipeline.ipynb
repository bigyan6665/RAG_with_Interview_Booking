{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6880c7d6",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3fd2f",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70075645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\GenAI_projects\\RAG_practice\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "def create_chunk(strategy:str) -> list[Document]:# the type annotations doesnot force to be followed\n",
    "    \"\"\"\n",
    "    Chunking based on user preference\n",
    "    Arguments:\n",
    "        strategy: name of the chunking strategy. Supported strategies = \"document\",\"recursive\"\n",
    "    Outputs:\n",
    "        list of documents/chunks\n",
    "    \"\"\"\n",
    "    if strategy == \"document\":\n",
    "        # document chunking method\n",
    "        pdf_dir_loader = DirectoryLoader(\n",
    "                                \"../data/booking_files\",\n",
    "                                glob=\"**/*.pdf\", # filename pattern \n",
    "                                loader_cls=PyMuPDFLoader, # loader class to use\n",
    "                                )\n",
    "        chunks = pdf_dir_loader.load()\n",
    "        print(f\"Chunks created: {len(chunks)}\")\n",
    "        return chunks\n",
    "\n",
    "    elif strategy == \"recursive\":\n",
    "        # recursive character chunking method\n",
    "        pdf_dir_loader = DirectoryLoader(\n",
    "                                \"../data/booking_files\",\n",
    "                                glob=\"**/*.pdf\", # filename pattern \n",
    "                                loader_cls=PyMuPDFLoader, # loader class to use\n",
    "                                )\n",
    "        docs = pdf_dir_loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,        # characters per chunk\n",
    "            chunk_overlap=200,      # overlap to preserve context\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "        chunks = text_splitter.split_documents(docs)\n",
    "        print(f\"Chunks created: {len(chunks)}\")\n",
    "        return chunks\n",
    "   \n",
    "    else:\n",
    "        return f\"chunking startegy not supported!!!!\"\n",
    "        # raise Exception(f\"{strategy} chunking startegy not supported!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a98afbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks created: 4\n"
     ]
    }
   ],
   "source": [
    "chunks = create_chunk(strategy=\"recursive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c90f6d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2026-01-07T17:51:25+05:45', 'source': '..\\\\data\\\\booking_files\\\\bigyan_shrestha(data_science)_cv.pdf', 'file_path': '..\\\\data\\\\booking_files\\\\bigyan_shrestha(data_science)_cv.pdf', 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'Bigyan Shrestha', 'subject': '', 'keywords': '', 'moddate': '2026-01-07T17:51:25+05:45', 'trapped': '', 'modDate': \"D:20260107175125+05'45'\", 'creationDate': \"D:20260107175125+05'45'\", 'page': 0}, page_content=\"BIGYAN SHRESTHA \\nData Science Intern | BIT | KIST college & SS \\nbigyans04@gmail.com | 9807904120 | Koteshwor, Kathmandu, Nepal \\n \\nINTRODUCTION \\nData science candidate having foundation in python, data analysis, machine \\nlearning and deep learning. Keen to apply my skills to extract insights from data \\nand help organizations to make data-driven decision-making. \\n \\nEDUCATION \\nKIST college & SS \\nBIT, 2022-2026 \\n \\nCERTIFICATIONS \\nBroadway Infosys \\nData Science With Python, 2025 \\n \\nPROJECTS \\nSales analytics dashboard \\n• Built a sales and deliveries dashboard analyzing 10 years of tesla's sales \\ndata.  \\n• Implemented filters for in-depth analysis. Link: \\nhttps://public.tableau.com/shared/CFZNQQ4SD?:display_count=n&:origin\\n=viz_share_link \\nExpenses-prediction \\n• Built a complete ML pipeline for predicting personal expenses with \\nautomated preprocessing, feature engineering, and hyperparameter-\\ntuned regression models. \\n• Evaluated multiple models using R² score, implemented centralized\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2026-01-07T17:51:25+05:45', 'source': '..\\\\data\\\\booking_files\\\\bigyan_shrestha(data_science)_cv.pdf', 'file_path': '..\\\\data\\\\booking_files\\\\bigyan_shrestha(data_science)_cv.pdf', 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'Bigyan Shrestha', 'subject': '', 'keywords': '', 'moddate': '2026-01-07T17:51:25+05:45', 'trapped': '', 'modDate': \"D:20260107175125+05'45'\", 'creationDate': \"D:20260107175125+05'45'\", 'page': 0}, page_content='automated preprocessing, feature engineering, and hyperparameter-\\ntuned regression models. \\n• Evaluated multiple models using R² score, implemented centralized \\nlogging and versioned artifact management for reproducibility. \\n• Deployed the best-performing model via a Flask web application enabling \\ntraining orchestration and real-time predictions. \\n• Source code: https://github.com/bigyan6665/ExpensesPrediction'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2026-01-07T17:51:25+05:45', 'source': '..\\\\data\\\\booking_files\\\\bigyan_shrestha(data_science)_cv.pdf', 'file_path': '..\\\\data\\\\booking_files\\\\bigyan_shrestha(data_science)_cv.pdf', 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'Bigyan Shrestha', 'subject': '', 'keywords': '', 'moddate': '2026-01-07T17:51:25+05:45', 'trapped': '', 'modDate': \"D:20260107175125+05'45'\", 'creationDate': \"D:20260107175125+05'45'\", 'page': 1}, page_content='Outlier transaction detection \\n• The pipeline for the entire outlier detection process was built which \\nincludes data cleaning, transformation, model training and evaluation. \\n• Local outlier factor and Isolation forest were implemented with \\nhyperparameter tuning and only  the best model was selected based on \\nsilhouette score. \\n• At last thorough analysis of the outliers was performed. \\n• The following github repository has the insights of that analysis: \\nhttps://github.com/bigyan6665/Outlier_Transaction_Detection \\nHotel recommendation system \\n• Built a system that recommends hotels for the user based on their past \\nratings history \\n• Implemented item based collaborative filtering with correlation \\ncoefficient \\n• Source code: https://github.com/bigyan6665/Hotel_recommendation \\n• Also build streamlit app for demo purpose. The app is live at: \\nhttps://hotel-recommendation-app.onrender.com   \\nNepali news classification'),\n",
       " Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2026-01-07T17:51:25+05:45', 'source': '..\\\\data\\\\booking_files\\\\bigyan_shrestha(data_science)_cv.pdf', 'file_path': '..\\\\data\\\\booking_files\\\\bigyan_shrestha(data_science)_cv.pdf', 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'Bigyan Shrestha', 'subject': '', 'keywords': '', 'moddate': '2026-01-07T17:51:25+05:45', 'trapped': '', 'modDate': \"D:20260107175125+05'45'\", 'creationDate': \"D:20260107175125+05'45'\", 'page': 1}, page_content='• Also build streamlit app for demo purpose. The app is live at: \\nhttps://hotel-recommendation-app.onrender.com   \\nNepali news classification \\n• Built a classification model to classify nepali-language news into topics like \\nsports, entertainment, politics, etc \\n• Applied logistic regression algorithm for training the model and also NLP \\ntechniques like stopwords removal, TF-IDF vectorization \\n• Source code: https://github.com/bigyan6665/Nepali_news_classification \\n• For app, visit: https://nepali-news-classification-app.onrender.com \\nSKILLS \\n• Languages: Python \\n• Frameworks & libraries: NumPy, Pandas, Matplotlib, Seaborn, Scikit-learn, \\nTensorFlow, Streamlit, FastAPI, Flask \\n• Databases: SQL \\n• Tools: Git, GitHub, Tableau')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380fb1f5",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "653e7ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Loaded embedding model successfully. Embedding Dimensions = <bound method SentenceTransformer.get_sentence_embedding_dimension of SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1ee32eb9e10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Embedding\n",
    "import numpy as np\n",
    "from  sentence_transformers import SentenceTransformer # this is our embedding model\n",
    "from typing import List\n",
    "\n",
    "class EmbeddingManager: \n",
    "    \"\"\"\n",
    "    handles document embedding generation using sentence transformer model\n",
    "    \"\"\"\n",
    "    def __init__(self,model_name:str=\"all-MiniLM-L6-v2\"): \n",
    "        \"\"\"\n",
    "        Constructor to initialize the EmbeddingManager\n",
    "        Arg = hugging face sentence transformer model name\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"\n",
    "        Load the specified sentence transformer model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Loaded embedding model successfully. Embedding Dimensions = {self.model.get_sentence_embedding_dimension}\")\n",
    "        except Exception as  e:\n",
    "            print(f\"Error loading model{self.model_name} = {e}\")\n",
    "\n",
    "    def generate_embeddings(self,texts:List[str]) -> np.array:\n",
    "        \"\"\"\n",
    "        Generates embeddings for list of text\n",
    "        Args: List of texts to embed\n",
    "        Return:numpy array with shape = (len(texts),embedding_dimensions)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        print(f\"Creating embeddings for {len(texts)} texts.\")\n",
    "        embeddings = self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Embeddings generated successfully with shape = {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef193b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[\"BIGYAN SHRESTHA \\nData Science Intern | BIT | KIST college & SS \\nbigyans04@gmail.com | 9807904120 | Koteshwor, Kathmandu, Nepal \\n \\nINTRODUCTION \\nData science candidate having foundation in python, data analysis, machine \\nlearning and deep learning. Keen to apply my skills to extract insights from data \\nand help organizations to make data-driven decision-making. \\n \\nEDUCATION \\nKIST college & SS \\nBIT, 2022-2026 \\n \\nCERTIFICATIONS \\nBroadway Infosys \\nData Science With Python, 2025 \\n \\nPROJECTS \\nSales analytics dashboard \\n• Built a sales and deliveries dashboard analyzing 10 years of tesla's sales \\ndata.  \\n• Implemented filters for in-depth analysis. Link: \\nhttps://public.tableau.com/shared/CFZNQQ4SD?:display_count=n&:origin\\n=viz_share_link \\nExpenses-prediction \\n• Built a complete ML pipeline for predicting personal expenses with \\nautomated preprocessing, feature engineering, and hyperparameter-\\ntuned regression models. \\n• Evaluated multiple models using R² score, implemented centralized\", 'automated preprocessing, feature engineering, and hyperparameter-\\ntuned regression models. \\n• Evaluated multiple models using R² score, implemented centralized \\nlogging and versioned artifact management for reproducibility. \\n• Deployed the best-performing model via a Flask web application enabling \\ntraining orchestration and real-time predictions. \\n• Source code: https://github.com/bigyan6665/ExpensesPrediction', 'Outlier transaction detection \\n• The pipeline for the entire outlier detection process was built which \\nincludes data cleaning, transformation, model training and evaluation. \\n• Local outlier factor and Isolation forest were implemented with \\nhyperparameter tuning and only  the best model was selected based on \\nsilhouette score. \\n• At last thorough analysis of the outliers was performed. \\n• The following github repository has the insights of that analysis: \\nhttps://github.com/bigyan6665/Outlier_Transaction_Detection \\nHotel recommendation system \\n• Built a system that recommends hotels for the user based on their past \\nratings history \\n• Implemented item based collaborative filtering with correlation \\ncoefficient \\n• Source code: https://github.com/bigyan6665/Hotel_recommendation \\n• Also build streamlit app for demo purpose. The app is live at: \\nhttps://hotel-recommendation-app.onrender.com   \\nNepali news classification', '• Also build streamlit app for demo purpose. The app is live at: \\nhttps://hotel-recommendation-app.onrender.com   \\nNepali news classification \\n• Built a classification model to classify nepali-language news into topics like \\nsports, entertainment, politics, etc \\n• Applied logistic regression algorithm for training the model and also NLP \\ntechniques like stopwords removal, TF-IDF vectorization \\n• Source code: https://github.com/bigyan6665/Nepali_news_classification \\n• For app, visit: https://nepali-news-classification-app.onrender.com \\nSKILLS \\n• Languages: Python \\n• Frameworks & libraries: NumPy, Pandas, Matplotlib, Seaborn, Scikit-learn, \\nTensorFlow, Streamlit, FastAPI, Flask \\n• Databases: SQL \\n• Tools: Git, GitHub, Tableau']\n",
      "4\n",
      "[{'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2026-01-07T17:51:25+05:45', 'source': '..\\\\data\\\\booking_files\\\\bigyan_shrestha(data_science)_cv.pdf', 'file_path': '..\\\\data\\\\booking_files\\\\bigyan_shrestha(data_science)_cv.pdf', 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'Bigyan Shrestha', 'subject': '', 'keywords': '', 'moddate': '2026-01-07T17:51:25+05:45', 'trapped': '', 'modDate': \"D:20260107175125+05'45'\", 'creationDate': \"D:20260107175125+05'45'\", 'page': 0}, {'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2026-01-07T17:51:25+05:45', 'source': '..\\\\data\\\\booking_files\\\\bigyan_shrestha(data_science)_cv.pdf', 'file_path': '..\\\\data\\\\booking_files\\\\bigyan_shrestha(data_science)_cv.pdf', 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'Bigyan Shrestha', 'subject': '', 'keywords': '', 'moddate': '2026-01-07T17:51:25+05:45', 'trapped': '', 'modDate': \"D:20260107175125+05'45'\", 'creationDate': \"D:20260107175125+05'45'\", 'page': 0}, {'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2026-01-07T17:51:25+05:45', 'source': '..\\\\data\\\\booking_files\\\\bigyan_shrestha(data_science)_cv.pdf', 'file_path': '..\\\\data\\\\booking_files\\\\bigyan_shrestha(data_science)_cv.pdf', 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'Bigyan Shrestha', 'subject': '', 'keywords': '', 'moddate': '2026-01-07T17:51:25+05:45', 'trapped': '', 'modDate': \"D:20260107175125+05'45'\", 'creationDate': \"D:20260107175125+05'45'\", 'page': 1}, {'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2026-01-07T17:51:25+05:45', 'source': '..\\\\data\\\\booking_files\\\\bigyan_shrestha(data_science)_cv.pdf', 'file_path': '..\\\\data\\\\booking_files\\\\bigyan_shrestha(data_science)_cv.pdf', 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'Bigyan Shrestha', 'subject': '', 'keywords': '', 'moddate': '2026-01-07T17:51:25+05:45', 'trapped': '', 'modDate': \"D:20260107175125+05'45'\", 'creationDate': \"D:20260107175125+05'45'\", 'page': 1}]\n",
      "Creating embeddings for 4 texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated successfully with shape = (4, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.06830518,  0.0603593 ,  0.01853083, ..., -0.067569  ,\n",
       "        -0.06738527,  0.04129963],\n",
       "       [-0.0511873 ,  0.01309703, -0.03875072, ..., -0.04730863,\n",
       "         0.01562451,  0.01251245],\n",
       "       [ 0.03273609, -0.02490785, -0.03396541, ..., -0.05335611,\n",
       "        -0.0343329 ,  0.09346751],\n",
       "       [-0.01059269, -0.06126603, -0.05471329, ...,  0.10227432,\n",
       "        -0.02142297,  0.07012384]], shape=(4, 384), dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = []\n",
    "chunk_metadata = []\n",
    "for chunk in chunks:\n",
    "    texts.append(chunk.page_content)\n",
    "    chunk_metadata.append(chunk.metadata)\n",
    "print(len(texts))\n",
    "print(texts)\n",
    "chunk_metadata = [chunk.metadata  for chunk in chunks]\n",
    "print(len(chunk_metadata))\n",
    "print(chunk_metadata)\n",
    "embeddings = embedding_manager.generate_embeddings(texts=texts)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3b42d",
   "metadata": {},
   "source": [
    "### Metadata Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab41b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pymysql.cursors\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv() # Loads variables from .env into os.environ\n",
    "\n",
    "\n",
    "class Metadata:\n",
    "    def __init__(self):\n",
    "        self.MYSQL_UID = os.getenv('MYSQL_UID')\n",
    "        self.MYSQL_PWD = os.getenv('MYSQL_PWD')\n",
    "        self.connection = None\n",
    "\n",
    "    def _create_connection(self):\n",
    "        self.connection = pymysql.connect(\n",
    "                host='localhost',\n",
    "                user=self.MYSQL_UID,\n",
    "                password=self.MYSQL_PWD,\n",
    "                database='rag_metadata',\n",
    "                cursorclass=pymysql.cursors.DictCursor\n",
    "            )\n",
    "        return self.connection\n",
    "    \n",
    "    def write(self,metadata):\n",
    "        connection = self._create_connection()\n",
    "        with connection:\n",
    "            with connection.cursor() as cursor:\n",
    "                for each in metadata:\n",
    "                    cols = list(each.keys())\n",
    "                    values = tuple(each.values())\n",
    "                    cols_q = \", \".join(cols)\n",
    "                    placeholders = \", \".join(['%s']*len(cols))\n",
    "                    # print(cols)\n",
    "                    # print(values)\n",
    "                    query = f\"INSERT INTO booking_rag_metadata ({cols_q}) VALUES ({placeholders})\"\n",
    "                    # print(query)\n",
    "                    cursor.execute(query,values)\n",
    "            connection.commit()\n",
    "        # print(metadata)\n",
    "\n",
    "    def delete_all(self):\n",
    "        connection = self._create_connection()\n",
    "        with connection:\n",
    "            with connection.cursor() as cursor:\n",
    "                query = f\"TRUNCATE TABLE booking_rag_metadata;\"\n",
    "                cursor.execute(query)\n",
    "            connection.commit()  \n",
    "\n",
    "    def write_booking_details(self,booking_details):\n",
    "        connection = self._create_connection()\n",
    "        with connection:\n",
    "            with connection.cursor() as cursor:\n",
    "                cols = list(booking_details.keys())\n",
    "                values = tuple(booking_details.values())\n",
    "                cols_q = \", \".join(cols)\n",
    "                placeholders = \", \".join(['%s']*len(cols))\n",
    "                # print(cols)\n",
    "                # print(values)\n",
    "                query = f\"INSERT INTO booking_details ({cols_q}) VALUES ({placeholders})\"\n",
    "                # print(query)\n",
    "                cursor.execute(query,values)\n",
    "            connection.commit()\n",
    "        # print(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bb89d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking_details = {\"name\":\"safaf\",\"email\":\"bfhef\",\"date\":\"dhfgd\",\"time\":\"dsfweyf\"}\n",
    "# Metadata().write_booking_details(booking_details=booking_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2636d7ca",
   "metadata": {},
   "source": [
    "### Vectore store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abc031cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1ee652e88e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datetime import datetime\n",
    "load_dotenv() # Loads variables from .env into os.environ\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self):\n",
    "        self.PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\") # access variables\n",
    "        self.PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "        self.PINECONE_HOST = os.getenv(\"PINECONE_HOST\")\n",
    "        pc = pinecone.Pinecone(\n",
    "            api_key=self.PINECONE_API_KEY\n",
    "        )\n",
    "        self.pinecone_index = pc.Index(host=self.PINECONE_HOST)\n",
    "\n",
    "    def store(self,embeddings,texts,chunk_metadata):\n",
    "        \"\"\"\n",
    "        function to store embedding vectors in pinecone\n",
    "        Argument:\n",
    "            embeddings: embedding vectors\n",
    "            texts: corresponding texts of embedding vectors\n",
    "        \"\"\"\n",
    "\n",
    "        vectors = []\n",
    "        metadata  = []\n",
    "        for i, embedding in enumerate(embeddings): # enumerate() lets you loop over items and get their index at the same time.\n",
    "            vectors.append(\n",
    "               {\n",
    "                \"id\": f\"doc-{i}\",\n",
    "                \"values\": embedding.tolist(),\n",
    "                \"metadata\": {\"text\": texts[i]}\n",
    "                } \n",
    "            )\n",
    "            metadata.append(\n",
    "                {\n",
    "                \"id\": f\"doc-{i}\",\n",
    "                \"uploaded_time\": datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),\n",
    "                \"source\": chunk_metadata[i]['source'] \n",
    "                }\n",
    "            )\n",
    "            \n",
    "        # writting the metadata in the database\n",
    "        Metadata().delete_all() # delete metadata\n",
    "        Metadata().write(metadata=metadata)\n",
    "        # self.pinecone_index.delete(delete_all=True)\n",
    "        self.pinecone_index.upsert(vectors=vectors)\n",
    "\n",
    "    def empty_index(self):\n",
    "        self.pinecone_index.delete(delete_all=True)\n",
    "        print(\"PineCone index is emptied\")\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ca0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore.empty_index()\n",
    "vectorstore.store(embeddings=embeddings,texts=texts,chunk_metadata=chunk_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d24fe7",
   "metadata": {},
   "source": [
    "# Conversational RAG Pipeline\n",
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c813b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List,Any,Dict,Tuple\n",
    "class RAGRetriever:\n",
    "    \"\"\"\n",
    "    Handles query based retrieval from vector store\n",
    "    \"\"\"\n",
    "    def __init__(self,vector_store:VectorStore,embedding_manager:EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        Args:\n",
    "        vector_store: vector store containing document embeddings\n",
    "        embedding_manager: manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query:str, top_k:int=3) -> Dict:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        Args:\n",
    "            query: the search query\n",
    "            top_k: no. of top results to return\n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: {query}\")\n",
    "\n",
    "        # generate query embeddings\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.pinecone_index.query(\n",
    "                vector=query_embedding.tolist(), \n",
    "                top_k=top_k,\n",
    "                include_metadata=True,\n",
    "                include_values=False\n",
    "            )\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval : {e}\")\n",
    "            return []\n",
    "        \n",
    "rag_retriever = RAGRetriever(vector_store=vectorstore,embedding_manager=embedding_manager)\n",
    "rag_retriever\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b5f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the retriever\n",
    "results = rag_retriever.retrieve(query=\"what is deep learning\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df227c2",
   "metadata": {},
   "source": [
    "## Augmentation and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55f3add",
   "metadata": {},
   "source": [
    "### Using redis for chat memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af248a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() # Loads variables from .env into os.environ\n",
    "REDIS_DB_USERNAME = os.getenv(\"REDIS_DB_USERNAME\") # access variables\n",
    "REDIS_DB_PASSWORD = os.getenv(\"REDIS_DB_PASSWORD\") # access variables\n",
    "\n",
    "SESSION_TTL_SECONDS = 500 # in seconds\n",
    "redis_client = redis.Redis(\n",
    "    host='redis-12530.c257.us-east-1-3.ec2.cloud.redislabs.com',\n",
    "    port=12530,\n",
    "    decode_responses=True,\n",
    "    username=REDIS_DB_USERNAME,\n",
    "    password=REDIS_DB_PASSWORD,\n",
    ")\n",
    "\n",
    "def get_chat_history(session_id: str = 1) -> list:\n",
    "    key = f\"chat:{session_id}\"\n",
    "    redis_client.expire(f\"chat:{session_id}\", SESSION_TTL_SECONDS) # sets/refreshes time to live period of the specified key\n",
    "    data = redis_client.lrange(key,0,-1)\n",
    "    return data if data else []\n",
    "\n",
    "def save_chat_history(history: dict, session_id: str = 1) -> None:\n",
    "    key = f\"chat:{session_id}\"\n",
    "    redis_client.expire(f\"chat:{session_id}\", SESSION_TTL_SECONDS) # sets/refreshes time to live period of the specified key\n",
    "    redis_client.rpush(key, json.dumps(history)) # pushes the additional history at the end of the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug and generation\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() # Loads variables from .env into os.environ\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\") # access variables\n",
    "\n",
    "def rag_simple(query,retriever,top_k=3):\n",
    "    # retrieve the context\n",
    "    results = retriever.retrieve(query=query,top_k=top_k)\n",
    "    results = [each['metadata'] for each in results['matches']]\n",
    "    context = \"\\n\\n\".join([each['text'] for each in results]) if results else \"\" # ternary operator not list comprehension with condition\n",
    "\n",
    "    # using redis for chat memory\n",
    "    history = get_chat_history()\n",
    "\n",
    "    # if not (context and history):\n",
    "    #     return \"No relevant context found for the question\"\n",
    "\n",
    "    # prompt for the groq model\n",
    "    booking_system_prompt = \"\"\"\n",
    "        Your job is to decide whether:\n",
    "         1. the user wants to book an interview or,\n",
    "         2. the user is asking a general question\n",
    "\n",
    "        If the user is asking a general question:\n",
    "        - Set route to \"rag\"\n",
    "        - Answer using the provided context and conversation history\n",
    "        - Put the answer in the \"reply\" field\n",
    "\n",
    "        If the user wants to book an interview:\n",
    "        - Set route to \"booking\"\n",
    "        - Extract the following fields:\n",
    "            - name\n",
    "            - email\n",
    "            - date\n",
    "            - time\n",
    "\n",
    "        Rules:\n",
    "        - Respond ONLY in valid JSON\n",
    "        - JSON fields must be exactly: route, booking, reply\n",
    "        - If a booking field is missing or unclear, set that field to null. Do NOT guess missing booking fields\n",
    "        - Convert dates to YYYY-MM-DD\n",
    "        - Convert times to 24-hour HH:MM\n",
    "        - If route is \"booking\", reply must be null\n",
    "        - If route is \"rag\", booking must be null\n",
    "        \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "            Conversation so far:\n",
    "            {history}\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Query:\n",
    "            {query}\n",
    "            \"\"\"\n",
    "    # print(prompt)\n",
    "    response = requests.post(\n",
    "    url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        # \"HTTP-Referer\": \"<YOUR_SITE_URL>\", # Optional. Site URL for rankings on openrouter.ai.\n",
    "        # \"X-Title\": \"<YOUR_SITE_NAME>\", # Optional. Site title for rankings on openrouter.ai.\n",
    "    },\n",
    "    data=json.dumps({\n",
    "        \"model\": \"stepfun/step-3.5-flash:free\",\n",
    "        # \"model\": \"liquid/lfm-2.5-1.2b-thinking:free\",\n",
    "        \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": booking_system_prompt},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "        ],\n",
    "        \"reasoning\": {\"enabled\": True}\n",
    "    })\n",
    "    )\n",
    "    print(response.json())\n",
    "    # print(type(json.loads(response.json())))\n",
    "    try:\n",
    "        output  = json.loads(response.json()['choices'][0]['message']['content'])\n",
    "        print(output)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"error: {e}\")\n",
    "\n",
    "    ## save_chat_history(history={\"user\":query,\"assistance\":output})\n",
    "    hist = {\"user\":query,\"assistance\":None}\n",
    "    if output['route'] == \"booking\":\n",
    "        if output[\"booking\"] is not None:\n",
    "            missing_fields = [k for k,v in output[\"booking\"].items() if v is None] # fields having none values\n",
    "            if len(missing_fields) != 0:\n",
    "                hist[\"assistance\"] = f\"Please provide the missing fields: {','.join(missing_fields)}\"\n",
    "                save_chat_history(history= hist)\n",
    "                return hist\n",
    "            else:\n",
    "                print(output['booking'])\n",
    "                Metadata().write_booking_details(output[\"booking\"])\n",
    "                hist[\"assistance\"] = \"Your interview is scheduled successfully\"\n",
    "                save_chat_history(history= hist)\n",
    "                # save the booking details\n",
    "                return hist\n",
    "        else:\n",
    "            all_fields = [\"name\",\"email\",\"date\",\"time\"]\n",
    "            hist['assistance'] = f\"Please provide the missing fields: {','.join(all_fields)}\"\n",
    "            save_chat_history(history= hist)\n",
    "            return hist\n",
    "    elif output[\"route\"] == \"rag\":\n",
    "        hist[\"assistance\"] = output[\"reply\"]\n",
    "        save_chat_history(history= hist)\n",
    "        return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to book an interview for the name of John and email is jhondoe45@gmail.com\n",
    "# print(rag_simple(query=\"what is deep learning?\",retriever=rag_retriever))\n",
    "# print(rag_simple(query=\"I want to book an interview\",retriever=rag_retriever))\n",
    "print(rag_simple(query=\"Name = Bigyan Shrestha, email = bigyans04@gmail.com, date = jan 25, 2024, time = 4 pm\",retriever=rag_retriever))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4a84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redis_client.delete(f\"chat:{session_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b7588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
